## Defective Convolutional Networks
Created by <a href="https://tiangeluo.github.io/" target="_blank">Tiange Luo</a>, <a href="https://tianle.website" target="_black">Tianle Cai</a>, <a href="" target="_blank">Mengxiao Zhang</a>, <a href="" target="_blank">Siyu Chen</a>, <a href="https://scholar.google.com/citations?user=orVoz4IAAAAJ&hl=en" target="_blank">Di He</a> and <a href="https://scholar.google.com/citations?user=VZHxoh8AAAAJ&hl=zh-CN" target="_blank">Liwei Wang</a>

<table width="100%" border="0" cellspacing="0" cellpadding="0">
  <tr>
    <td align="center"><img src="./imgs/original.png" width="233" height="233" /> </td>
    <td align="center"><img src="./imgs/attacked.png" width="233" height="233" /></td>
  </tr>
  <tr>
    <td align="center">The origin image labeled as Truck</td>
    <td align="center">The adversarial example(\ell_\infty = 16) predicted by the model as Frog</td>
  </tr>
</table>


## Introduction
This repository is code release for this [paper](https://openreview.net/forum?id=E8fmaZwzEj). Chinese patent #<a href="https://www.vipzhuanli.com/patent/202110577422.5/"> 202110577422.5</a>.

A new kind of CNNs that makes predictions relying less on textural information but more on shape information, where shape cues are more robust against various distortions introduced in the imaging process. As a result, compared to standard CNNs, the proposed CNNs show better defense performance against black-box attacks and transfer performance on the Stylized-ImageNet.

## Citation
If you find our work or codes useful in your research, please consider citing:

    @article{luo2019defective,
          title={Defective Convolutional Networks},
          author={Luo, Tiange and Cai, Tianle and Zhang, Mengxiao and Chen, Siyu and He, Di and Wang, Liwei},
          journal={arXiv preprint arXiv:1911.08432},
          year={2019}
    }

## Requirements
- Python: 2.7.14
- PyTorch: 0.4.1
- CUDA: 9.0

## Codes Organization

- Attacks: implementations of attack methods including FGSM, PGD, MIFGSM, CW, and boundray attacks. The codes will generate adversarial examples, and test the white-box defense performance (FGSM, PGD, MIFGSM, CW), and the black-box defense performance against decision-based attack. Also, we have compared our implementations with the implementation from Github.

- Defenses: codes to evaluate the black-box defense performance against the adversarial examples generated by other models. Using the implementations under **Attacks** folder to generate adversarial examples. Running the codes under **Mains** folder to train the models.

- Mains: codes for training models on CIFAR-10 dataset. codes are named as main_xxx_drop_ttt_pp.py, where xxx means the network archtecture or used methods, ttt means the position to apply defective convolutional layers, and pp means the (1 - keep probability). If xxx is omitted, the code train a model based on ResNet-18.

- Masks: codes for generating masks of defective neurons. codes are named as genmask_xxx_ttt_pp.py, where xxx means the network archtecture, ttt means the position to apply defective convolutional layers, and pp means the (1 - keep probability). If xxx is omitted, the code generates a mask for ResNet-18.

- MNIST: codes for MNIST experiments. See the paper for detailed settings.

- Models: various networks including all standard CNNs and their defective version. codes are named as xxx_drop_ttt_pp.py, where xxx means the network archtecture, ttt means the position to apply defective convolutional layers, and pp means the (1 - keep probability).

- RandomShuffle: codes for the random shuffle experiments. See the paper for detailed settings.

- RandomNoise: codes for the random noise experiments. See the paper for detailed settings.
