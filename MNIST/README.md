##Illustration

- Attacks: implementations of PGD. The codes will generate adversarial examples and test the white-box defense performance.

- Defenses: codes to evaluate the black-box defense performance against the adversarial examples generated by other models. Using the implementations under **Attacks** folder to generate adversarial examples. Running the codes under **Mains** folder to train the models.

- Mains: codes of main function for training models on MNIST dataset.

- Masks: codes for generate mask of defective neurons.

- Models: various networks including all standard CNNs and their defective version.

